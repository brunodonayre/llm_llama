{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f7054f-0ffd-43af-84c3-cb8ab4f4fcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.53.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.12/site-packages (1.8.1)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.12/site-packages (0.16.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.12/site-packages (0.46.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets accelerate peft bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e12e94-f5f0-442b-aa29-5db72ac808fa",
   "metadata": {},
   "source": [
    "# Ir a https://www.llama.com/llama-downloads/ para registrarse a Llama para descargar el modelo mas conveniente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6766d8-f1c4-4b14-96a8-5fcaacb3e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack in /opt/conda/lib/python3.12/site-packages (0.2.14)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.11.16)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.115.14)\n",
      "Requirement already satisfied: fire in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.7.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.33.2)\n",
      "Requirement already satisfied: jinja2>=3.1.6 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from llama-stack) (4.23.0)\n",
      "Requirement already satisfied: llama-stack-client>=0.2.14 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.2.14)\n",
      "Requirement already satisfied: openai>=1.66 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (1.93.0)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.0.50)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (from llama-stack) (1.1.1)\n",
      "Requirement already satisfied: python-jose in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.5.0)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (2.11.3)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from llama-stack) (14.0.0)\n",
      "Requirement already satisfied: starlette in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.46.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.0.1)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.9.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from llama-stack) (11.1.0)\n",
      "Requirement already satisfied: h11>=0.16.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.16.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.20 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.0.20)\n",
      "Requirement already satisfied: uvicorn>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk in /opt/conda/lib/python3.12/site-packages (from llama-stack) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http in /opt/conda/lib/python3.12/site-packages (from llama-stack) (1.34.1)\n",
      "Requirement already satisfied: aiosqlite>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.21.0)\n",
      "Requirement already satisfied: asyncpg in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.30.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /opt/conda/lib/python3.12/site-packages (from aiosqlite>=0.21.0->llama-stack) (4.13.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (1.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2>=3.1.6->llama-stack) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (4.9.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (8.1.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (1.9.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (2.2.3)\n",
      "Requirement already satisfied: pyaml in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (25.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx->llama-stack) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx->llama-stack) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx->llama-stack) (3.10)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.66->llama-stack) (0.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->llama-stack) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->llama-stack) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (1.18.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->llama-stack) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema->llama-stack) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->llama-stack) (0.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http->llama-stack) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-api~=1.15 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http->llama-stack) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http->llama-stack) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http->llama-stack) (1.34.1)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-http->llama-stack) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-sdk->llama-stack) (0.55b1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-http->llama-stack) (8.6.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
      "Requirement already satisfied: ecdsa!=0.15 in /opt/conda/lib/python3.12/site-packages (from python-jose->llama-stack) (0.19.1)\n",
      "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /opt/conda/lib/python3.12/site-packages (from python-jose->llama-stack) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.5.0 in /opt/conda/lib/python3.12/site-packages (from python-jose->llama-stack) (0.6.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->llama-stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->llama-stack) (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken->llama-stack) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from ecdsa!=0.15->python-jose->llama-stack) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.30.0->llama-stack) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.30.0->llama-stack) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.2.14->llama-stack) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.2.14->llama-stack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.2.14->llama-stack) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.2.14->llama-stack) (2025.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-http->llama-stack) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fea2a5-10a3-41d5-ba47-36e4e307b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-stack in /opt/conda/lib/python3.12/site-packages (0.2.14)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.11.16)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.115.14)\n",
      "Requirement already satisfied: fire in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.7.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.33.2)\n",
      "Requirement already satisfied: jinja2>=3.1.6 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.12/site-packages (from llama-stack) (4.23.0)\n",
      "Requirement already satisfied: llama-stack-client>=0.2.14 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.2.14)\n",
      "Requirement already satisfied: openai>=1.66 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (1.93.0)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.0.50)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (from llama-stack) (1.1.1)\n",
      "Requirement already satisfied: python-jose in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.5.0)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (2.11.3)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from llama-stack) (14.0.0)\n",
      "Requirement already satisfied: starlette in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.46.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.12/site-packages (from llama-stack) (3.0.1)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.9.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from llama-stack) (11.1.0)\n",
      "Requirement already satisfied: h11>=0.16.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.16.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.20 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.0.20)\n",
      "Requirement already satisfied: uvicorn>=0.34.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk in /opt/conda/lib/python3.12/site-packages (from llama-stack) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http in /opt/conda/lib/python3.12/site-packages (from llama-stack) (1.34.1)\n",
      "Requirement already satisfied: aiosqlite>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.21.0)\n",
      "Requirement already satisfied: asyncpg in /opt/conda/lib/python3.12/site-packages (from llama-stack) (0.30.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /opt/conda/lib/python3.12/site-packages (from aiosqlite>=0.21.0->llama-stack) (4.13.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->llama-stack) (1.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2>=3.1.6->llama-stack) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (4.9.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (8.1.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (1.9.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (2.2.3)\n",
      "Requirement already satisfied: pyaml in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (25.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from llama-stack-client>=0.2.14->llama-stack) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx->llama-stack) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx->llama-stack) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx->llama-stack) (3.10)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from openai>=1.66->llama-stack) (0.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->llama-stack) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2->llama-stack) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->llama-stack) (1.18.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.12/site-packages (from jsonschema->llama-stack) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.12/site-packages (from jsonschema->llama-stack) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from jsonschema->llama-stack) (0.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http->llama-stack) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-api~=1.15 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http->llama-stack) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http->llama-stack) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http->llama-stack) (1.34.1)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-http->llama-stack) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-sdk->llama-stack) (0.55b1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-http->llama-stack) (8.6.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
      "Requirement already satisfied: ecdsa!=0.15 in /opt/conda/lib/python3.12/site-packages (from python-jose->llama-stack) (0.19.1)\n",
      "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /opt/conda/lib/python3.12/site-packages (from python-jose->llama-stack) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.5.0 in /opt/conda/lib/python3.12/site-packages (from python-jose->llama-stack) (0.6.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->llama-stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->llama-stack) (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken->llama-stack) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from ecdsa!=0.15->python-jose->llama-stack) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.30.0->llama-stack) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.30.0->llama-stack) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.2.14->llama-stack) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.2.14->llama-stack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.2.14->llama-stack) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->llama-stack-client>=0.2.14->llama-stack) (2025.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.15->opentelemetry-exporter-otlp-proto-http->llama-stack) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-stack -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e61dad-2755-40d8-b25f-2564bc18bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mModel Descriptor(ID)        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mHugging Face Repo           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mContext Length\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-8B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-8B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-70B                \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-70B    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B:bf16-mp8      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B               \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-F…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B:bf16-mp16     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-8B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-8B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-70B-Instruct       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-70B-In…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct:bf16…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct:bf16…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-11B-Vision         \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-11B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-90B-Vision         \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-90B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct:int4-q…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct:int4-s…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct:int4-q…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct:int4-s…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-11B-Vision-Instruct\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-11B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-90B-Vision-Instruct\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-90B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.3-70B-Instruct       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.3-70B-In…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Scout-17B-16E       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Scout-17…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m256K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m256K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Scout-17B-16E-Instr…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Scout-17…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m10240K        \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m1024K         \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m1024K         \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-4-12B           \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-4-12B\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-11B-Vision    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-11…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-1B:int4       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-1B…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-1B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-1B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-8B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-8B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-8B:int8       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-8B…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-2-8B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-2-8B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "└──────────────────────────────┴──────────────────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!llama model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e90071-f97e-4ca6-b450-1ac0cfe6b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mModel Descriptor(ID)        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mHugging Face Repo           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mContext Length\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-7b                  \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-7b       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-13b                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-13b      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-70b                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-70b      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-7b-chat             \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-7b-chat  \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-13b-chat            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-13b-chat \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-2-70b-chat            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-2-70b-chat \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-3-8B                  \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3-8B       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-3-70B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3-70B      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-3-8B-Instruct         \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3-8B-Instr…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-3-70B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3-70B-Inst…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-8B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-8B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-70B                \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-70B    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B:bf16-mp8      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B               \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-F…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B:bf16-mp16     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-8B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-8B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-70B-Instruct       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-70B-In…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct:bf16…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct      \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.1-405B-Instruct:bf16…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.1-405B-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B                 \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B     \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-11B-Vision         \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-11B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-90B-Vision         \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-90B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct        \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct:int4-q…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-1B-Instruct:int4-s…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-1B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct:int4-q…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-3B-Instruct:int4-s…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-3B-Ins…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-11B-Vision-Instruct\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-11B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.2-90B-Vision-Instruct\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.2-90B-Vi…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama3.3-70B-Instruct       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-3.3-70B-In…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Scout-17B-16E       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Scout-17…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m256K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E   \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m256K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Scout-17B-16E-Instr…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Scout-17…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m10240K        \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m1024K         \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-4-Maverick-17B-128E-I…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-4-Maverick…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m1024K         \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-4-12B           \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-4-12B\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m8K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-11B-Vision    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-11…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-1B:int4       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-1B…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-1B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-1B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-8B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-8B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-3-8B:int8       \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-3-8B…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m128K          \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Guard-2-8B            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Guard-2-8B \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m4K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mPrompt-Guard-86M            \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Prompt-Guard-86M \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m0K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Prompt-Guard-2-86M    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Prompt-Gua…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m0K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "├──────────────────────────────┼──────────────────────────────┼────────────────┤\n",
      "│\u001b[1;37m \u001b[0m\u001b[1;37mLlama-Prompt-Guard-2-22M    \u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37mmeta-llama/Llama-Prompt-Gua…\u001b[0m\u001b[1;37m \u001b[0m│\u001b[1;37m \u001b[0m\u001b[1;37m0K            \u001b[0m\u001b[1;37m \u001b[0m│\n",
      "└──────────────────────────────┴──────────────────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!llama model list --show-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27e70a01-bf7c-41f2-bbe9-c26011c20316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: llama [-h] {model,stack,download,verify-download} ...\n",
      "llama: error: unrecognized arguments: --insecure\n"
     ]
    }
   ],
   "source": [
    "!llama model download --source meta --model-id  Llama3.2-3B --meta-url \"https://llama3-2-lightweight.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiaXQ2cHBoeHpna2Fhejhxa3h0OGk0Z3VpIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWEzLTItbGlnaHR3ZWlnaHQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTkzOTUzMn19fV19&Signature=YSB6umuYUOsj559VPVKW8349wsUWSvsILQMAFtApGfEs5sfULp7ui98TfaNmEe7SD7B%7EaVrLcqmeS-SUMN07kpa8ldk97Om4yOMtKzLXX%7Em3escyy3VPGUNoZQl4haZyujtyb8NA-Ln6w4CBUluln6qKHgU6aPCDvkDJND4gugyRFsnOfFunVuh02cw6sBRWDzjyoz0NeJUqs7dC%7EHlsdve3a7-yHqJeeOIRGoMDRK%7E2tXLoy8WlRpX5OJjTZTsM2DFOM7k8PrlBV2pJGkMzWu-TgmiNHkVw217ErpYKhBXYEnSy9K9-AIwVyMygk%7EdOrRZ%7Ese60pLvwf2CnA5cQcw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=648992861487744\" --insecure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a7ac5-c689-4d4a-bcda-a8e68a0cc3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the signed URL for model Llama3.2-3B you received via email after visiting https://www.llama.com/llama-downloads/ (e.g., https://llama3-1.llamameta.net/*?Policy...): "
     ]
    }
   ],
   "source": [
    "!llama model download --source meta --model-id  Llama3.2-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddc5fff-b9a6-4d29-abad-f5774d8d5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.12/site-packages (0.33.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9482d9f-60df-4642-bba2-0a4cfe4b7626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751771388.842385    1156 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751771388.845854    1156 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751771388.854644    1156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751771388.854653    1156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751771388.854654    1156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751771388.854655    1156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f657ad8594db47019ed3be34ca78fe17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546f8de4213a46ca8642ec70ff6620ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e33b55d6154598a7fef316612333b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Failed to close writer for Some(\"/home/jovyan/.cache/huggingface/xet/https___cas_serv-tGqkUaZf_CBPHQ6h/chunk-cache/R5/R5zjoZRUZX1lm4uECPZfM_OhmmsEJCOmKRhKoo4dqT5kZWZhdWx0/IwIAAEUDAADeHhMBAAAAAMMEL44=\"): No space left on device (os error 28)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Data processing error: CAS service error : IO Error: No space left on device (os error 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     25\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m     26\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     27\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     bnb_4bit_compute_dtype=torch.float16,\n\u001b[32m     29\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\u001b[32m     40\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py:4680\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4671\u001b[39m     gguf_file\n\u001b[32m   4672\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4673\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4674\u001b[39m ):\n\u001b[32m   4675\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4676\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4677\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4678\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4680\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4682\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4687\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4693\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4698\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4700\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4701\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/modeling_utils.py:1295\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1293\u001b[39m sharded_metadata = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     checkpoint_files, sharded_metadata = \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1309\u001b[39m     checkpoint_files = [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/utils/hub.py:1110\u001b[39m, in \u001b[36mget_checkpoint_shard_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[32m   1108\u001b[39m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[32m   1109\u001b[39m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1110\u001b[39m cached_filenames = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/utils/hub.py:557\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m     \u001b[38;5;66;03m# Any other Exception type should now be re-raised, in order to provide helpful error messages and break the execution flow\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# (EntryNotFoundError will be treated outside this block and correctly re-raised if needed)\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    559\u001b[39m resolved_files = [\n\u001b[32m    560\u001b[39m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    561\u001b[39m ]\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# If there are any missing file and the flag is active, raise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/utils/hub.py:485\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    470\u001b[39m         hf_hub_download(\n\u001b[32m    471\u001b[39m             path_or_repo_id,\n\u001b[32m    472\u001b[39m             filenames[\u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    482\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    483\u001b[39m         )\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    500\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py:327\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    325\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/_snapshot_download.py:301\u001b[39m, in \u001b[36msnapshot_download.<locals>._inner_hf_hub_download\u001b[39m\u001b[34m(repo_file)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_inner_hf_hub_download\u001b[39m(repo_file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:1161\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1174\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:1710\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[32m   1709\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1710\u001b[39m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1716\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1717\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1718\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:627\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m    625\u001b[39m     progress.update(progress_bytes)\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Data processing error: CAS service error : IO Error: No space left on device (os error 28)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")  # Replace with your token\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B\"  # Adjust based on the exact LLaMA 3.1 model name\n",
    "DATASET_PATH = \"../Datos/spanish_chatbot_dataset.json\"\n",
    "OUTPUT_DIR = \"llama3-spanish-chatbot\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Quantization (4-bit + CPU offload)\n",
    "# The BitsAndBytesConfig is used for quantizing large language models (LLMs) to make them more memory-efficient \n",
    "# during training and inference. Below is a detailed breakdown of each hyperparameter in your configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Loads the model in 4-bit precision instead of the usual 32-bit (FP32) or 16-bit (FP16/BF16). This reduces memory usage by ~8x compared to FP32.\n",
    "    bnb_4bit_quant_type=\"nf4\", # Specifies the 4-bit quantization algorithm. Here, \"nf4\" stands for NormalFloat 4-bit, an optimized format for normally distributed weights \n",
    "    bnb_4bit_compute_dtype=torch.float16, # Sets the compute dtype for operations (e.g., matrix multiplications) to FP16. While weights are stored in 4-bit, computations happen in 16-bit for better numerical stability.\n",
    "    bnb_4bit_use_double_quant=True, # Enables double quantization, which quantizes the quantization constants (saving even more memory).\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "\n",
    "# LoRA (Low-Rank Adaptation) Config\n",
    "# LoRA is a parameter-efficient fine-tuning (PEFT) method that freezes the original large language model (LLM) and \n",
    "# injects small trainable low-rank matrices into specific layers. This allows fine-tuning with far fewer parameters\n",
    "# (often <1% of the original model size) while maintaining performance.\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, # Rank of the low-rank matrices\n",
    "    lora_alpha=32, # Scaling factor for LoRA weights\n",
    "    lora_dropout=0.05, # Dropout probability for LoRA layers\n",
    "    bias=\"none\", # Whether to train bias parameters\n",
    "    task_type=\"CAUSAL_LM\",  # Task type (causal language modeling)\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Modules to apply LoRA to: [\"q_proj\", \"v_proj\"] applies LoRA to the query and value projections in transformer attention layers.\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Basic Training Setup\n",
    "    output_dir=OUTPUT_DIR, #Directory where the trained model checkpoints and logs will be saved.\n",
    "    num_train_epochs=3, # Number of full passes through the dataset (3 epochs here).\n",
    "    per_device_train_batch_size=4, # Batch size per GPU/CPU. Higher values require more memory.\n",
    "    gradient_accumulation_steps=2, # Splits a large batch into smaller chunks to save memory.\n",
    "    # Optimization & Learning Rate\n",
    "    optim=\"paged_adamw_32bit\", # Uses a memory-efficient version of AdamW optimizer (good for large models like Llama).\n",
    "    learning_rate=2e-4, # Initial learning rate. A common starting point for fine-tuning LLMs.\n",
    "    weight_decay=0.001, # L2 regularization to prevent overfitting (penalizes large weights).  \n",
    "    fp16=True, # Uses mixed-precision training (16-bit floats) to reduce memory usage and speed up training.\n",
    "    bf16=False, # Disables BFloat16 precision (use bf16=True if your hardware supports it, e.g., modern GPUs/TPUs).\n",
    "    max_grad_norm=0.3, # Clips gradients to prevent exploding gradients (limits the norm to 0.3).\n",
    "    warmup_ratio=0.03, # Gradually increases LR from 0 to 2e-4 over 3% of training steps (avoids early instability).\n",
    "    lr_scheduler_type=\"cosine\", # Learning rate schedule: smoothly decreases LR in a cosine curve for better convergence.\n",
    "    # Efficiency & Logging\n",
    "    save_steps=500, # Saves a model checkpoint every 500 steps.\n",
    "    group_by_length=True, # Groups similar-length sequences together to minimize padding and improve efficiency.\n",
    "    logging_steps=10,  # Logs training metrics (loss, LR, etc.) every 10 steps.\n",
    "    report_to=\"tensorboard\", # Logs metrics to TensorBoard for visualization (alternatives: \"wandb\", \"mlflow\").\n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"no\", # Disables evaluation during training (set to \"steps\" or \"epoch\" if you have a validation set).\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"output\",  # Using 'output' as our training text\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Training complete! Model saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437a804-4c2b-46c1-9321-54a5137e3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Configuration\n",
    "PEFT_MODEL_ID = \"llama3-spanish-chatbot\"  # Path to your saved model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "base_model = \"meta-llama/Llama-3.2-3B\"  # Original base model you fine-tuned from\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the fine-tuned PEFT model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL_ID)\n",
    "model = model.merge_and_unload()  # Merge LoRA adapters with base model\n",
    "\n",
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Test cases\n",
    "test_prompts = [\n",
    "    \"¿Cuál es la capital de España?\",  # What is the capital of Spain?\n",
    "    \"Explícame el concepto de inteligencia artificial\",  # Explain AI\n",
    "    \"Recomiéndame un libro interesante\",  # Recommend a book\n",
    "    \"¿Cómo puedo aprender programación?\",  # How to learn programming\n",
    "    \"Háblame sobre la historia de México\"  # Tell me about Mexico's history\n",
    "]\n",
    "\n",
    "# Generate responses\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n=== Prompt: {prompt} ===\")\n",
    "    \n",
    "    # Generate response\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    print(\"Response:\", output[0]['generated_text'])\n",
    "    \n",
    "    # Optional: Calculate perplexity (measure of model confidence)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        perplexity = torch.exp(outputs.loss).item()\n",
    "        print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "# Optional: Interactive chat mode\n",
    "print(\"\\n=== Interactive Mode (type 'quit' to exit) ===\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "        \n",
    "    output = pipe(\n",
    "        user_input,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"Bot:\", output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
